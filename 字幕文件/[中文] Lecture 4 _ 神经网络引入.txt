[学生喃喃自语]
-好了，大家下午好，我们开始吧。
嗨，对于那些我还没见过的人，
我叫杨小琳，我是第三名
也是这门课的最后一位讲师，
我也是飞飞小组的博士生。
好了，今天我们要谈谈反向传播
和神经网络，所以现在我们真的开始
来了解这门课的一些核心内容。
在我们开始之前，让我们看看，哦。
所以一些行政细节，
所以作业一的截止日期是4月20日星期四，
提醒一下，我们把日期往后移了一点
画布上的时间是晚上11:59。
所以你应该开始考虑你的项目，
Piazza网站上有列出TA的专业
因此，如果您对某个特定的项目主题有疑问
你在想，你可以去试着找到
最相关的助教。
还有谷歌云，所以所有学生都要去
获得100美元的信用点数来使用谷歌云
对于他们的任务和项目，
所以你应该会收到一封邮件
我想这一周就是为了这个。
你们很多人可能已经有了，然后
对于你们中还没有来的人来说，他们会来的，
应该在这个周末。
好了，我们到目前为止已经讨论了
如何使用函数f定义分类器，
由权重W参数化，这个函数f
将数据x作为输入，输出一个分数向量
对于要分类的每个类别。
因此从这里我们也可以定义
一个损失函数，例如，SVM损失函数
我们已经讨论过的基本上量化了
我们有多快乐或不快乐
我们制作的乐谱，对吧，
然后我们可以用它来定义总损失项。
这里的L是这个数据项的组合，
结合正则化项，表示
我们的模型是多么简单，我们有一个偏好
为了更简单的模型，为了更好的概括。
所以现在我们想找到参数W
这相当于我们最低的损失，对吗？
我们想最小化损失函数，
为此我们想找到
L相对于w的梯度。
上节课我们谈到了我们如何做到这一点
利用最优化，我们要去
在这个方向上反复采取步骤
最陡下降，这是梯度的负值，
为了走过这个失落的风景
并达到最低损失点，对吗？
我们看到这种梯度下降基本上是如何进行的
这条轨迹，看起来像右边的这张图片，
弄清你的损失状况。
哦！
好的，我们也讨论了不同的方式
来计算梯度，对吗？
我们可以用数字来计算
使用有限差分近似
这是缓慢而近似的，但同时
写出来真的很容易，
你知道你总是可以这样得到梯度。
我们还讨论了如何使用解析梯度
计算这是，它很快
一旦你得到了表达式
解析梯度，但同时你也有
做所有的数学和微积分推导出这个，
所以也很容易犯错，对吧？
所以在实践中，我们想要做的是
解析梯度并使用它，
但同时使用以下命令检查我们的实现
数值梯度来确保
我们所有的数学都是正确的。
所以今天我们要讨论如何计算
任意复杂函数的解析梯度，
使用一个我称之为计算图的框架。
所以基本上计算图是什么，
我们可以用这种图表来排序
来表示任何函数，图中的节点
是我们所经历的计算步骤。
例如，在这个例子中，
我们讨论过的线性分类器，
这里的输入是x和W，对吧，
然后这个乘法节点代表
矩阵乘法器，的乘法
参数W和我们的数据x，
输出我们的分数向量。
然后我们有了另一个计算节点
代表我们的铰链损耗，对吧，
计算我们的数据丢失项，李。
我们也有这个正则项
右下角，这个节点
计算我们的正则项，
最后我们的总损失，L，
是正则项和数据项的总和。
好处是一旦我们可以表达
使用计算图形的函数，
然后我们可以使用一种叫做反向传播的技术
它将递归地使用链式法则
为了计算梯度
对于计算图中的每个变量，
所以我们要看看这是怎么做到的。
当我们开始工作时，这变得非常有用
非常复杂的函数，
例如，卷积神经网络
我们将在这节课稍后讨论。
我们在顶部有输入图像，
我们的损失在底部，
输入必须经过许多层
来获得所有的
通向损失函数的路。
这可能会变得更加疯狂，
你知道，就像一个神经图灵机，
这是另一种深度学习模式，
在这种情况下，你可以看到计算图
因为这太疯狂了，尤其是，
我们最终，你知道，随着时间的推移展开这个。
基本上完全不切实际
如果你想计算梯度
对于任何一个中间变量。
好吧，那么反向传播是如何工作的？
所以我们从一个简单的例子开始，
我们的目标是有一个函数。
所以在这种情况下，x，y，z的f
等于x加y乘以z，
我们想找到输出的梯度
关于任何变量的函数。
所以第一步，总是，我们想要
取我们的函数f，我们想要
用计算图来表示它。
好的，我们的计算图在右边，
你可以看到我们有我们的，
首先我们有正节点，所以x+y，
然后我们有了这个倍增节点，对吧，
我们正在做的第二个计算。
然后，现在我们要向前传球
所以给定这个网络的值
我们拥有的变量，所以在这里，
x等于负二，y等于五
z等于-4，我要把这些都填上
在我们的计算图中，然后我们可以计算
一个中间值，所以x加y等于3，
最后我们再次通过它，
通过最后一个节点，乘法运算，
得到f的最终节点等于-12。
所以这里我们要给每个中间变量一个名字。
所以这里我称这个中间变量为
正节点q，q等于x加y，
然后f等于q乘以z，用这个中间节点。
我也在这里写下了，梯度
q相对于x和y的位置，也就是一个
因为加法，然后f的梯度
关于q和z，分别是z和q
因为乘法法则。
所以我们想要找到的，是我们想要找到的
f相对于x，y和z的梯度。
所以backprop是什么，它是一个递归应用
链式法则，所以我们从后面开始，
计算图的最末端，
然后，我们将反向工作
并计算沿途的所有梯度。
所以如果我们从最末端开始，对吧，
我们想计算输出的梯度
关于最后一个变量，也就是f。
这个梯度只有一个，很小。
所以现在，向后移动，我们想要梯度
关于z，对，我们知道
df除以dz等于q。
所以q值只有3，
所以我们这里有，df除以dz等于3。
接下来如果我们想对dq做df，
那有什么价值？
什么是df超过dq？
所以我们这里有，df除以dq等于z，对吧，
z的值是负四。
这里df除以dq等于负四。
好的，现在继续回到图表上，
我们想找到df而不是dy，对吧，
但是在这种情况下，相对于y的梯度，
y和f不直接相连，对吗？
它通过中间节点z连接，
所以我们要做的是
我们可以利用链式法则
dy上的df可以写成dq上的df，
乘以dq除以dy，这就是直觉
为了找出y对f的影响，
这实际上相当于如果我们
q乘以q对f的影响，这个我们已经知道了，对吧？
df除以dq等于负四，
我们用y对q，dq对dy的影响来复合它。
那么在这种情况下dq除以dy等于什么呢？
-[学生]一。
-一个，对。没错。
所以dq除以dy等于1，这意味着，你知道，
如果我们把y改变一点点，
q将会改变大约
同样的数量对，这就是效果，
所以这是在说，
如果我把y改变一点点，
y对q的影响是1，
那么q对f的影响大约是
负四的因子，对吗？
然后我们把这些相乘
我们得到y对f的影响
将会是负四。
好，现在如果我们想做同样的事情
相对于x的梯度，右边，
我们可以做，我们可以遵循同样的程序，
那么这将会是什么呢？
[学生远离麦克风讲话]
-我也听到了。
是的，没错，所以在这种情况下，我们想，再一次，
应用链式法则，对吗？
我们知道q对f的影响是负四，
这里，因为我们也有相同的加法节点，
dq除以dx也等于1，
我们有负四乘以一，对吧，还有梯度
相对于x是负四。
好的，所以我们正在做的是，在反推中，
我们基本上有所有这些节点
在我们的计算图中，但是每个节点
只意识到它的周围环境，对吗？
所以在每个节点，我们都有本地输入
连接到这个节点上，
流入节点的值，
然后我们还有输出
直接从这个节点输出。
这里我们的本地输入是x和y，输出是z。
在这个节点上，我们也知道局部梯度，对吧，
我们可以计算z相对于x的梯度，
以及z相对于y的梯度，
这些通常都是非常简单的操作，对吗？
每个节点都像是
加法或乘法
在之前的例子中，
这是我们可以写下来的东西
梯度，我们不需要，你知道，
通过非常复杂的微积分来找到这个。
-[学生]你能回去解释一下为什么吗
上一张幻灯片中有更多与规划不同的地方
它的第一部分仅仅使用普通的微积分？
-是的，所以基本上如果我们回去，
等等，让我来...
所以如果我们回到这里，我们可以准确地写出，
用微积分找到所有这些，
所以我们可以说，我们希望df大于dx，对吧，
我们可以扩展这个表达式
看到它只是z，
但是我们可以这样做，在这种情况下，
因为这很简单，但我们稍后会看到例子
一旦这变成一个非常复杂的表达，
你不希望必须使用微积分
去推导，对，某样东西的梯度，
对于一个超级复杂的表达式，
相反，如果你使用这种形式主义
你把它分解成这些计算节点，
那么你只能使用渐变
非常简单的计算，对吧，
你知道，在加法，乘法的水平上，
指数，像你想要的那样简单的东西，
然后你就用链式法则
将所有这些相乘，
得到你的梯度值
而不必推导出完整的表达式。
这有意义吗？
[学生喃喃自语]
好的，我们稍后会看到一个例子。
所以，还有另一个问题吗？
[学生远离麦克风讲话]
-[学生]什么是负四
旁边的z代表什么？
-否定的，好吧是的，所以负四，
这些是上面的绿色值
是我们经过的函数的所有值
它通过计算图向前，对吗？
所以我们在这里说x等于-2，
y等于5，z等于负4，
所以我们填入所有这些值，然后我们只是想
来计算这个函数的值。
对，所以我们说q的值是x加y，
它将会是负二加五，它会
是三，我们有z等于负四
所以我们把它填在这里，然后乘以q
和z加在一起，负四乘以三
为了得到f的最终值，对吗？
下面的红色值
当我们填充渐变时
当我们往回走的时候。
好吧。
好吧，好吧，我们说过，你知道，
我们有这些本地的，这些节点，
每个节点基本上都有本地输入
以及它看到的直接传递的输出
到下一个节点，我们也有这些局部梯度
我们计算了，对，的梯度
节点的直接输出
关于输入的信息。
在反投影过程中，我们有这些东西，
我们从图表的后面开始，对吧，
然后我们从最后开始
一路回到起点，
当我们到达每个节点时，在每个节点我们都有
上游梯度回来了，对吧，
关于节点的直接输出。
所以当我们到达后向投影的这个节点时，
我们已经计算了梯度
最终损失l，相对于z，对吗？
所以现在我们下一步想找的是
我们想找到尊重的梯度
到节点之前，到x和y的值。
正如我们之前看到的，我们使用链式法则，
对，我们有链式法则，
这个损失函数的梯度
相对于x将会是
相对于z时间的梯度，由下式合成
这个梯度，z相对于x的局部梯度。
对，所以在链式法则中，我们总是
上游坡度下降，
我们把它乘以局部梯度
以便获得相对于输入的梯度。
-[学生]那么，对不起，是吗，
这是不同的，因为这永远不会工作
为了得到一个通用公式，
或梯度的通用符号公式。
它只适用于瞬时值，
你喜欢的地方。
[学生咳嗽]
或者传递一个小的常量值作为符号。
-所以问题是这是否只起作用
因为我们正在研究的是
这个函数，所以它是有效的，对吧，
给定我们插入的函数的当前值，
但是我们可以写一个表达式，
仍然是变量，对吗？
所以我们会看到L相对于z的梯度
会有一些表达式，和z的梯度
关于x是另一个表达式，对吗？
但是我们插入这些，我们插入值
为了得到这些数字
相对于x的梯度值。
所以你可以做的是递归插入
所有这些表达，对吗？
z相对于x的梯度
会是一个简单的表达式，对吗？
所以在这种情况下，如果我们有一个乘法节点，
z相对于x的梯度
是的，我们知道，
但是L相对于z的梯度，
这可能是一个复杂的部分
图表本身，对，所以我们想在这里，
在这种情况下，有这个数值，对不对？
所以正如你所说，基本上这将是公正的
一个数字，对，一个值，
然后我们把它乘以
局部梯度的表达式。
我想当我们经历了这一切后，这一点会更加清楚
几张幻灯片中的一个更复杂的例子。
好的，现在L相对于y的梯度，
我们有完全相同的想法，
我们使用链式法则，我们有L的梯度
关于z，乘以z的梯度
关于y，我们使用链式法则，
将这些相乘，得到我们的梯度。
一旦我们有了这些，我们会把它们传递给
直接在此节点之前的节点，或连接到此节点的节点。
因此，从这里我们学到的主要东西是
在每个节点上，我们只需要局部梯度
我们计算，只要跟踪这个，
然后在我们接收信号时，你知道，
来自上游的梯度数值，
我们只是把它乘以
局部梯度，然后这是
然后我们发送回连接的节点，
下一个节点倒着走，不必在意
除了这些直接的环境。
现在我们来看另一个例子，
这次稍微复杂一点，
所以我们可以更多地了解为什么backprop如此有用。
所以在这种情况下，我们的函数是w和x的f，
等于1/1加上e
w-0乘以x-0的负值
加上w-1 x-1，加上w-2，对吗？
所以，第一步总是我们想要
把它写成一个计算图。
在这种情况下，我们可以在这个图表中看到，对吧，
首先我们将w和x项相乘，
w-0和x-0，w-1和x-1，
和w-2，然后我们把这些加在一起，对吗？
然后我们用负一来衡量它，
我们取指数，加上一，
最后我们整个学期都会做一次。
然后在这里我也填入了这些值，
假设我们有给定的值
对于ws和xs，我们可以向前传球
基本上计算出它的价值
在计算的每个阶段。
我还在底部写下了
一些导数的值和表达式
这对以后会有帮助，
和我们之前做的简单例子一样。
好的，现在我们要从这里开始反向投影，
好的，所以我们再次从
图表的最末端，这里也是
输出相对于最后一个变量的梯度
只是一个，它只是微不足道的，
所以现在后退一步，对吗？
那么相对于
就在1/x之前的输入？
在这种情况下，我们知道上游梯度
这是红色的，对吗？
这是我们向下流动的上游梯度，
现在我们需要找到局部梯度，对吧，
以及这个节点的局部梯度，
这个节点是x的1分之一，对吧，
所以我们用红色表示x的f等于1除以x，
以及这个df对dx的局部梯度
等于负一除以x的平方，对吗？
所以这里我们要用负一除以x的平方，
代入x的值我们在
这个向前传球，1.37，所以我们最后的梯度
关于这个变量的情况
负1/1.37的平方乘以1
等于负0.53。
所以回到下一个节点，
我们会经历完全相同的过程，对吗？
这里，从上游流出的梯度
会是-0.53，对吧，
这里是局部梯度，这里的节点是正1，
现在来看看我们对导数的引用
底部是常数加x，
局部梯度只有一个，对吧？
那么尊重的梯度是多少
对这个变量使用链式法则。
所以这将是上游梯度
负0.53倍的局部梯度，
等于负0.53。
所以让我们继续后退一步。
这里我们有指数，对吗？
那么上游的梯度是多少呢？
[学生远离麦克风讲话]
对，所以上游梯度是负0.53，
这里的局部梯度是多少？
这是e对x的局部梯度，对吗？
这是一个指数节点，所以我们的链式法则
会告诉我们我们的梯度
会是负0.53乘以e的x次方，
在这种情况下是负的，
从我们的向前传球，这将会给我们
我们的最终梯度为负0.2。
好了，现在这里多了一个节点，
我们到达的下一个节点是
与负一相乘，对吗？
那么在这里，上游的坡度是多少？
-[学生]负0.2？
-[瑟琳娜]负0.2，对吧，那会是什么
局部梯度，可以看一下参考表。
会是什么来着？
我想我听到了。
-[学生]那是负一？
-这将是负一，确切地说，是啊，
因为我们的局部梯度表示它将会是，
df除以dx是a，对，a的值
我们乘以x的值是负的。
所以我们这里有梯度
是负一乘以负0.2，
所以我们的梯度是0.2。
好了，现在我们到了一个加法节点，
在这种情况下，我们有两个分支
两者都有关联，对吗？
那么这里的上游梯度是多少？
它会是0.2，对吧，就像其他东西一样，
这里是关于尊重的梯度
对每一个分支来说，这是一个附加，对吧，
我们在前面的简单例子中看到
当我们有一个额外的节点时，
相对于每个输入的梯度
加起来只有一个，对吗？
这里，我们观察顶部水流的局部梯度
将是上游梯度的一倍
0.2，总梯度是0.2，对吗？
然后我们，对于我们的底部分支，我们会做
同样的事情，对，我们的上游梯度是0.2，
我们的局部梯度也是1，
总梯度为0.2。
这一切都清楚了吗？
好吧。
所以我们还有一些渐变要填，
现在我们回到w-0和x-0，
这里我们有一个乘法节点，对吧，
我们看到了之前的乘法节点，
它只是，梯度与尊重
其中一个输入的值就是另一个输入的值。
在这种情况下，梯度是多少
关于w-zero？
-[学生]负0.2。
-负，我听到负0.2，正是。
是的，所以关于w-zero，
我们有上游梯度，0.2，对吧，
乘以我们的，这是最下面的，
乘以我们的x值，它是负一，
我们得到负0.2，我们可以做同样的事情
相对于x-0的梯度。
它将是w-0值的0.2倍
也就是2，我们得到0.4。
好了，这里我们已经填好了大部分的渐变，
所以之前有个问题
为什么这比单纯的计算简单，
推导解析梯度，表达尊重
这些变量中的任何一个，对吗？
所以你可以在这里看到，我们所处理的
局部梯度的was表达式
所以一旦我们有了这些表达式
对于局部梯度，我们所做的
把我们现有的每一个值都代入其中，
用链式法则把这些数字相乘
倒着走，得到梯度
关于所有的变量。
所以，你知道，我们也可以填写
这里关于w-1和x-1的梯度
以完全相同的方式，所以有一件事
我想指出的是，当我们创造
这些计算图表，我们可以定义
我们想要的任何粒度的计算节点。
所以在这种情况下，我们把它分解成
我们能做的最简单的事，对吧，
我们把它分解成加法和乘法，
你知道，基本上没有比这更简单的了，
但是实际上，我们可以把一些
如果我们愿意，可以把这些节点组合成更复杂的节点。
只要我们能够写下
该节点的局部梯度，对吗？
举个例子，如果我们看一个sigmoid函数，
所以我在中定义了sigmoid函数
这里的右上角，是x的s形
等于1/1加上e的负x次方，
这是一个非常常见的功能
在这节课的剩余部分你会看到很多，
我们可以计算这个的梯度，
我们可以把它写出来，如果我们真的完成了
从分析的角度来看，
我们可以在最后得到一个很好的表达。
所以在这种情况下，它等于x的负一σ，
所以这个函数的输出乘以x的sigma，对吗？
所以在这种情况下，
我们可以把所有的计算
我们在构成这个乙状结肠的图表中，
我们可以替换它
有一个很大的节点是s形的，对吧，
因为我们知道这个门的局部梯度，
是这个表达式，x除以dx的sigmoid的d，对吗？
所以基本上重要的是
您可以根据需要对任何节点进行分组
来制造任何稍微复杂一点的节点，
只要你能写下这个的局部梯度。
所以这一切基本上是一种权衡，
你知道，你想做多少数学运算
为了得到一个更简洁的图表，
对，相对于你想要多简单
你的每一个梯度，对不对？
然后你可以写出复杂的
你想要的计算图。
嗯，有问题吗？
-[学生]这是图表本身的问题，
前两个乘法节点
并且权重不连接到单个加法节点？
-所以它们也可以连接到
一个加法节点，所以问题是，
w-零和x-zero有什么原因吗
与w-2无关。
所有这些附加物连接在一起，
是的，原因是，答案是
如果你愿意，你可以这样做，
实际上，也许你真的想这么做
因为这还是一个很简单的节点，对吧？
所以在这种情况下，我只是把它写成简单的
在每个节点最多只有两个输入的情况下，
但是，是的，你绝对可以这样做。
关于这个还有其他问题吗？
好吧，我真正喜欢的一件事是
像计算图一样思考这个问题
我感到很安慰，对吧，
就像任何时候我需要取一个梯度，
寻找某物的渐变，即使表达式
我想计算的梯度是非常棘手的，
真的很可怕，你知道，不管是像
这个乙状结肠或者更糟的，
我知道，你知道，如果我想的话，我可以得到这个，
但是真的，如果我坐下来写出来
从计算图的角度来看，
我可以想怎么简单就怎么简单
为了总是能够应用反向投影和链式法则，
并且能够计算出我需要的所有梯度。
所以这是你们应该思考的事情
当你做作业的时候，基本上，你知道，
任何时候，当你找不到东西的梯度时
把它想象成一个计算图，
把它分解成所有这些部分，
然后用链式法则。
好的，所以，你知道，所以我们讨论了
我们如何将这些节点集合在一起
为了确认一下，就像，
这实际上是完全等价的，
我们可以插上这个，对吧？
所以我们在这里输入到sigmoid门
将会是一个绿色的，然后我们有
输出会在这里，0.73，对吧，
如果你插上插头，它就会工作
它在乙状结肠函数中。
所以现在如果我们想做，如果我们想拿
梯度，我们想治疗整个乙状结肠
作为一个节点，现在我们应该做什么
我们需要使用这个局部梯度
我们在这里得到的，对吗？
一减去x的sigmoid乘以x的sigmoid。
所以如果我们把这个插上，我们就知道了
x的sigmoid值是0.73，
所以如果我们把这个值代入我们会看到，
这个梯度的值等于0.2，对，
所以这个局部梯度的值是0.2，
我们把它乘以x上游梯度，也就是1，
我们会得到完全相同的值
相对于s形门之前的梯度，
就好像我们把它分解成所有更小的计算。
好的，当我们看到发生了什么，对吧，
当我们把这些梯度向后推时
通过我们的计算图表，
你会注意到一些模式
有一些直观的解释
我们可以给他们这些，对吗？
我们看到加法门是一个梯度分配器，
当我们通过这个加法门时，
它有两个分支，
它采用了梯度，上游梯度
它只是分发它，传递完全相同的东西
两个相连的分支。
这里有几个我们可以思考的问题。
max gate是什么样子的？
所以我们在底部有一个最大的门，对吧，
输入是z和w，
z的值为2，w的值为负1，
然后我们取最大值，也就是2，对吧，
所以我们把这个传递给其他人
我们的计算图。
所以现在如果我们对这个取梯度，
上游梯度是，比如说两个回来，对吧，
这个局部梯度是什么样的？
所以任何人，是吗？
-[学生]一个是零，另一个是一？
-没错。
[学生远离麦克风讲话]
没错，所以给出的答案是
z的梯度为2，
w会有一个值，梯度为零，
所以其中一个会得到
梯度的全值刚刚传回，
路由到那个变量，然后是另一个
梯度为零，所以，
所以我们可以把它想象成一个梯度路由器，对吧，
因此，当加法节点返回时
两条支流的坡度相同，
最大值门将只取梯度
并把它传送到其中一个分支，
这是有道理的，因为如果我们看看我们的向前传球，
现在的情况是只有价值
这是传下来的最大值
计算图的其余部分，对吗？
所以这是唯一真正影响
我们的函数计算在最后，所以它是有意义的
当我们把梯度传回来的时候，
我们只是想调整一下，你知道，
让它流过计算分支。
好，那么另一个，什么是乘法门，
我们之前看到的，对此有什么解释吗？
[学生远离麦克风讲话]
好的，给出的答案是
局部梯度基本上只是
另一个变量的值。
是的，所以这是完全正确的。
所以我们可以把它想象成一个渐变切换器，对吗？
一个切换器，我猜是一个缩放器，我们把它
上游梯度，我们用
另一个分支的值。
好的，还有一点需要注意的是
当我们有一个节点
连接到多个节点，
梯度在这个节点累加，对吗？
所以在这些分支，使用多元链式法则，
我们只需要取
从每个节点返回的上游梯度，
我们将把这些加在一起得到总数
回流到这个节点的上游梯度，
你可以从多元链式法则中看到这一点
想想这个，你可以想想这个
如果你要稍微改变这个节点，
它会影响这两个相连的节点
在向前传球的时候，当你
你向前穿过图表。
所以当你做反投影时，对吧，
现在，这两个梯度都回来了
会影响这个节点，对吧，
这就是我们要总结的
回流到此节点的总上游坡度。
好吧，关于backprop有什么问题吗，
经历这些前后传球？
-[学生]所以我们什么也没做
来更新权重。
[远离麦克风讲话]
-对，所以问题是，我们还没有做任何事情
为了更新这些权重的值，
我们只发现了尊重的梯度
对变量来说，完全正确。
到目前为止，我们在这节课中讨论的是
计算任何变量的梯度
在我们的函数中，对吧，一旦我们有了这些
我们可以应用我们在
最优化讲座，上节课，对吧？
所以给定梯度，我们现在进入一步
按顺序渐变的方向
来更新我们的体重和参数，对吗？
所以你可以把这整个框架
我们在上节课学到了最优化，
我们在这里所做的只是学习如何计算
我们需要任意复杂函数的梯度，
对，所以这在我们谈话的时候会很有用
像神经网络这样的复杂功能。
是吗？
-[学生]你介意写下，
所有的变量，所以你可以帮助解释
这张幻灯片好点了吗？
-是的，所以我可以把这个写在黑板上。
好的，所以基本上如果我们要，让我想想，
如果我们要得到f的梯度
关于某个变量x，对，
假设它通过变量联系在一起，
我想想，我，我们基本上可以...
对，所以这基本上是说
如果x和这些元素相连，对吧，
在这种情况下，不同的q-1，
那么链式法则就是把所有的，
它将会影响到每一个
这些中间变量，对吧，
在我们的最终输出f上，然后将每一个与
变量x对的局部影响
中间值，对吗？
所以，是的，基本上就是把所有这些加在一起。
好了，现在我们已经，你知道，做了所有这些例子
在标量的情况下，我们要看看
当我们有矢量时会发生什么，对吗？
所以现在如果我们的变量x，y和z，
不仅仅是数字，我们还有向量。
所以一切都保持不变，整个流程，
唯一的区别是现在我们的梯度
会是雅可比矩阵，对吧，
所以这些矩阵包含了
每个元素的导数，例如z
关于x的每个元素。
好的，举个例子
一些正在发生的事情，对吧，比方说
我们的输入现在是一个矢量，
假设我们有一个4096维的输入向量，
这是你可能会看到的常见尺寸
在后来的卷积神经网络中，
我们的节点将是元素最大值，对吗？
所以x的f等于最大值
与零元素相比，然后我们的输出
也是一个4096维的向量。
好的，那么在这种情况下，尺寸是多少
我们的雅可比矩阵？
记得我之前说过，雅可比矩阵
就像每一排一样，
它会是偏导数，
的每个维度的偏导数的矩阵
相对于输入的每个维度的输出。
好吧，我听到的答案是4096的平方，
是的，没错。
这很大，对吧，4096乘4096
实际上这个数字会更大
因为我们要和许多批次一起工作
比如说，100个输入
同时，对，我们会把所有这些
通过我们的节点来提高效率，
这将把它放大100倍，
实际上我们的雅克比实际上会变成
大概是409，000乘409，000对吧，
所以这真的很重要，基本上
完全不切实际。
所以实际上，我们并不需要
为了计算这个巨大的雅可比矩阵，
那么为什么会这样呢
这个雅可比矩阵看起来像什么？
如果我们想想这里发生了什么，
我们取这个元素的最大值，
我们思考每一个
偏导数，对吧，
输入的哪个维度会产生影响
输出的哪些维度？
在雅可比矩阵中我们能看到什么样的结构？
[学生远离麦克风讲话]
好吧，我听说是对角线，对，没错。
因为这是元素方面的，对吧，每个元素
输入，比如说第一维，只影响
输出中相应的元素，对吗？
因此我们的雅可比矩阵，
这将是一个对角矩阵。
所以实际上，我们并没有
写出并阐明整个雅可比矩阵，
我们可以知道x对输出的影响，对吧，
然后我们可以用这些值，对吧，
在我们计算梯度的时候把它填上。
好了，现在我们要穿过
计算图的一个更具体的矢量化例子。
好，我们来看一个案例
这里我们有x和W的函数f
等于，基本上是W的L-2乘以x，
所以在这种情况下我们会说x
是n维的，W是n乘n。
好的，所以我们的第一步，
写出计算图，对吗？
我们将W乘以x，然后再乘以，
我把这个叫做L-2。
现在让我们也为它填写一些值，
所以我们可以看到，你知道，让我们说有W be
这个2乘2的矩阵，还有x
会是这个二维向量，对吗？
所以我们可以说，再次标记我们的中间节点。
所以我们的中间节点在倍增后
会是q，我们有q等于W乘以x，
我们可以这样按元素写出，
其中第一个元素就是W-1-1乘以x-1
加上W-1-2乘以x-2等等，
然后我们现在可以表示f和q的关系，对吗？
所以看第二个节点，我们有q的f
等于q的L-2范数，
等于q-1的平方加上q-2的平方。
好吧，我们填了这个，对吧，
我们得到q，然后得到最终输出。
好的，现在让我们反推一下，好吗？
所以，这总是第一步，
相对于输出的梯度只有一个。
好的，现在让我们向后移动一个节点，
现在我们想找到相对于q的梯度，
对，我们在L-2之前的中间变量。
所以q是一个二维向量，
我们想要做的是
为了找出q的每个元素如何影响f的最终值，
对，所以如果我们看这个表达式
我们在底部写下了f，
我们可以看到f的梯度
对于一个特定的q-i，比如说q-one，
将会是q-i的两倍，对吗？
这只是在这里求导，
所以我们有这样一个表达，
关于q-1的每个元素，
我们也可以，你知道，把这个写出来
如果我们想的话，
它将会是我们矢量q的两倍，对吧，
如果我们想用向量的形式写出来，
所以我们得到的梯度是0.44，
这个向量是0.52，对吗？
所以你可以看到它只需要q
它把它放大了两倍，对吗？
每个元素只是乘以2。
所以矢量的梯度总是
与原始向量的大小相同，
这个梯度的每一个元素，
意思是这种特殊元素的含量
会影响函数的最终输出。
好吧，现在让我们后退一步，对吧，
相对于W的梯度是多少？
所以这里我们想再次使用相同的概念
尝试应用链式法则，对吧，
所以我们想计算我们的局部梯度
q相对于W的角度，让我们看看
如果我们这样做的话，
让我们看看每个q的影响是什么，对吧，
q的每个元素相对于W的每个元素，
这将是雅可比矩阵
我们之前讨论过的，如果我们看看这个
在这个乘法中，q等于
W乘以x，对，导数是多少，
或者q的第一个元素的梯度，
所以我们上面的第一个元素，相对于W-1-1？
所以q-1相对于W-1-1？
那值多少？
一点没错。
是的，所以我们知道这是x-one，
我们可以写出更一般的
q-k相对于W-i，j的梯度等于X-j。
如果我们想找到梯度
关于f，关于每个W-i，j。
所以现在看看这些衍生品，
我们可以用我们之前讲过的链式法则
我们基本上是将df除以dq-k
对于q的每个元素，dq-k除以W-i，j
对于W-i，j的每一个元素，对吗？
所以我们找到了W的每个元素的影响
对q的每个元素求和。
所以如果你把它写出来，这将会给出
这个2乘以q-i乘以x-j的表达式。
好的，填完这个我们就可以
这个相对于W的梯度，
所以我们可以计算每一个元素，
或者我们也可以看看我们推导出的这个表达式
并以矢量化的形式写出来，对吗？
好吧，记住，重要的是
总是检查相对于变量的梯度
应该和变量有相同的形状，
所以这在实践中非常有用
来进行健全性检查，对吧，就像你计算过
你的梯度应该是多少，检查一下
和你的变量形状一样，
因为这个元素，你渐变的每一个元素
量化这种元素的含量
影响你的最终产出。
是吗？
[学生远离麦克风讲话]
两边，哦两边一个
是一个指标函数，所以这是说
如果k等于I就是1。
好的，让我们看看，我们已经完成了，
现在再看一个例子。
现在我们最不需要找到的是
相对于q-I的梯度。
如果我们计算偏导数，我们可以看到
dq-k除以dx-i等于W-k，I，对，
用和W一样的方法，
然后我们可以用链式法则
得到它的总表达式，对吗？
这就是梯度
关于x，同样，与x形状相同，
我们也可以把这个写出来
如果我们想的话，可以用矢量化的形式。
好吧，有什么问题吗？
[学生远离麦克风讲话]
所以我们正在计算雅可比矩阵，
让我回到这里，好的，
所以如果我们在做，那么对，那么我们有
这些q-k的偏导数
关于x-i，对，还有这些
形成了你的雅可比项，对吗？
所以实际上我们要做的是
我们基本上拿着它，你会看到它
在链式法则中，向量表达式
关于x的梯度，右边，
这里会有雅可比矩阵
也就是这个转置的值，
所以你可以用矢量化的形式写出来。
[学生远离麦克风讲话]
那么，在这种情况下，矩阵
会和W右一样大，
所以在这种情况下它实际上不是一个大矩阵，对吗？
好吧，我们思考这个问题的方式
就像一个真正模块化的实现，对吧，
在我们的计算图中，
我们局部观察每个节点，然后计算
局部渐变并链接它们
随着上游梯度下降，
所以你可以认为这基本上是
向前和向后API，对吗？
在向前传球中，我们实现了，
计算该节点输出的函数，
然后在向后的过程中，我们计算梯度。
所以当我们用代码实现它的时候，
我们要用完全一样的方法。
所以我们基本上可以考虑，对于每个门，对吧，
如果我们实现一个向前函数和一个向后函数，
在反向函数计算链式法则的情况下，
如果我们有了完整的图表，我们就可以
通过迭代向前遍历整个图
图中所有的节点，所有的门。
这里我要用“门”和“节点”这个词，
某种程度上可以互换，我们可以迭代
所有这些门，只要向前呼叫
在每个门上，对吗？
我们只是想按照拓扑排序的顺序来做这件事，
所以我们处理所有输入到
在我们处理那个节点之前。
然后往回走，我们就这样
以相反的顺序通过所有的门，
然后在每个门上向后调用。
好的，如果我们看看
我们特定门的实现，
举个例子，这个乘法运算，
我们想实现向前传球，对吧，
所以它得到x和y作为输入，并返回z的值，
当我们倒退时，对，
我们得到输入dz，这是我们的上游梯度，
我们想要输出梯度
把输入的x和y传递下去，对吗？
所以我们要输出dx和dy，
所以在这个例子中，
这里一切都回到标量的情况，
所以如果我们向前看，
重要的一点是我们需要，
我们应该缓存向前传递的值，对吧，
因为我们最终会用这个
落后的传球很多时候。
所以在正向传递中，我们希望缓存这些值
x和y，对，在向后的过程中，
记住，利用链式法则，
取上游梯度的值
用另一个分支的值来换算，对吧，
所以我们会保持，对于dx，我们会取我们的值
我们保留的自我，并将其倍增
当dz下降时，dy也一样。
好的，如果你看很多深度学习框架
和图书馆，你会看到他们完全遵循
这种模块化，对吧？
例如，Caffe是一个流行的深度学习框架，
你会看到，如果你去看看Caffe的源代码
你会找到一个目录，上面写着层，
在基本上是计算节点的层中，
通常层数会稍微多一点，你知道，
一些更复杂的计算节点
就像我们之前提到的乙状结肠一样，
你会看到，基本上只是一个完整的列表
各种不同的计算节点，对吗？
所以你可能有乙状结肠，我知道
这里可能有一个卷积，
还有一个Argmax是另一层，
你会有所有这些层次，如果你深入研究
对他们每个人来说，他们只是在实现
向前传球和向后传球，
所有这些都叫做
当我们向前和向后传球时
通过我们建立的整个网络，
所以我们的网络基本上是
堆积所有这些东西，
我们选择在网络中使用的不同层。
举个例子，如果我们看一个特定的例子，
在这种情况下，你会看到一个sigmoid层
在乙状结肠层，对吧，
我们已经讨论过乙状结肠函数，
你会看到有一个向前传球
它基本上精确地计算了sigmoid表达式，
然后向后传球，对，
它接受一些输入，基本上是一个top_diff，
这是我们的上游梯度，
乘以我们计算的局部梯度。
所以在作业一中你会得到练习
用这种计算图形的思维方式
在那里，你知道，你将会写作
你的SVM和软最大类，
取这些的梯度。
所以，请记住，你总是想迈出第一步，
用计算图来表示，对吗？
找出所有的计算
你在输出之前所做的，
然后当你，当时间到了
要向后传球，只需利用梯度
对于这些中间变量中的每一个
你已经在你的计算图中定义了，
并用链式法则把它们联系起来。
好了，总结一下我们到目前为止讨论的内容。
当我们开始研究神经网络时，
这些将会非常庞大和复杂，
所以写下来是不切实际的
手动计算所有参数的梯度公式。
所以为了得到这些梯度，对吧，
我们讨论了如何，我们应该使用反向传播，
对，这是核心技术之一
你知道，神经网络，基本上是
用反向传播来得到你的梯度，对吗？
这是一个递归应用
我们有这个计算图的链式法则，
我们从后面开始，然后往回走
来计算相对于
所有的中间变量，
哪些是你的输入，你的参数，
其他的都在中间。
我们还讨论了
这个实现和这个图形结构，
你可以看到，这些节点中的每一个
实现向前和向后的API，对吗？
所以在向前传递的过程中，我们想要计算
手术的结果，我们想要
保存任何中间值
我们可能会在以后的梯度计算中用到，
然后在向后的过程中，我们应用这个链式法则
我们采用这个上游梯度，
我们把它链起来，乘以我们的局部梯度
计算相对于的梯度
节点的输入，我们把它传递下去
到下一个连接的节点。
好了，现在我们终于要走了
来讨论神经网络。
好吧，所以真的，你知道，神经网络，
人们在神经网络之间做了很多类比
和大脑，以及不同的类型
生物灵感，我们会在
有一点，但首先让我们谈谈，你知道，
仅仅把它看作一种功能，
作为一类没有大脑的功能。
到目前为止，我们已经讨论过了，你知道，
我们已经用这个线性得分函数做了很多工作，对吗？
f等于W乘以x，所以我们一直用这个
作为我们想要优化的函数的运行示例。
所以在你的转换中不用单曲，
如果我们想要一个神经网络
作为最简单的形式，
把两个叠在一起，对吗？
只是另一个之上的线性变换
为了得到一个两层的神经网络，对吗？
所以这看起来像是首先我们有我们的，你知道，
W-one与x矩阵乘法，
然后我们得到这个中间变量
我们有这个最大值为零的非线性函数
用这个线性层的输出W，max，
拥有这些非线性真的很重要
我们稍后会详细讨论，
因为否则如果你只是在顶部堆叠线性层
他们会崩溃的
就像一个线性函数。
好了，我们有了第一个线性图层
然后我们有了这种非线性，对吧，
然后在此之上，我们将添加另一个线性层。
然后从这里，最终我们可以得到我们的得分函数，
分数的输出向量。
所以基本上，更广义地说，
神经网络是一类函数
我们有更简单的功能，对吧，
一个堆叠在另一个上面，
我们以一种分层的方式将它们堆叠起来
为了组成更复杂的非线性函数，
这是一个有多个阶段的想法
分层计算，对吗？
所以，你知道，这是一种
我们做到这一点的主要方法是
像这个矩阵乘法，这个线性层，
我们把多个这样的东西堆叠在一起
中间有非线性函数，对吗？
这可以帮助解决的一件事是，如果我们观察，
如果我们回想起这个线性得分函数
我们讨论过的，对吧，
还记得我们之前讨论的每行如何
权重矩阵W就像一个模板。
这是一个模板，你知道，
我们在输入中寻找什么
对于一个特定的类，对吧，所以举例来说，你知道，
汽车模板看起来像这样
这种模糊的红色汽车，我们正在寻找这个
在输入中计算汽车类的分数。
我们讨论了其中的一个问题
只有一个模板，对吗？
有一辆红色的车，然而实际上，
我们实际上有多种模式，对吗？
我们可能想要，我们在找，你知道，一辆红色的车，
还有一辆黄色的车，和所有这些一样
不同种类的汽车，那又怎样
这种多层网络让你做
现在，你知道，每个中间变量h，
是的，我们仍然可以使用这些模板，
但是现在你有了所有这些分数
对于h中的这些模板，我们可以有另一层
最重要的是把这些结合在一起，对吗？
所以我们可以说，实际上我的汽车类应该是，
你知道，连接到，我们正在寻找
红色汽车和黄色汽车都适用，对吧，
因为我们有这个矩阵W-2
现在是h中所有向量的权重。
好吧，有什么问题吗？
是吗？
[学生远离麦克风讲话]
是的，所以有很多方法，
所以有很多不同的非线性函数
你可以从中选择，我们稍后再谈
在后面的一节课中，我们会讲到所有不同的种类
你可能会用到的非线性。
-[学生]对于幻灯片中的图片，
所以，最下面一行是图片
你的向量W-1的权重，所以也许
你会有另一个向量W-2的图像？
-所以W-one，因为它直接连接到
输入x，这是可以解释的，
因为你可以制定所有这些模板。
w-2，所以h是分数
例如，你解决了多少个模板，
好吧，就像你有一个，你知道的，
就像，我不知道，两辆红色的车，
就像，一辆黄色的车或类似的东西。
-[学生]哦，好吧，所以不是W-one只有10岁，
比如，你会有一匹朝左的马
和一匹朝右的马，它们都包括在内-
-没错，所以问题基本上是
在W-one中，你可以拥有两个面向左边马
向右的马，对，所以没错。
所以现在W-one可以有很多不同种类的模板，对吗？
他们不是，然后W-2，现在我们可以，基本上
它是所有这些模板的加权总和。
因此，现在它允许您将多个模板加权在一起
为了得到某一门课的最终分数。
-[学生]如果你在处理一幅图像
那么实际上是左面马。
它会得到很高的分数
朝左的马模板，
右向马模板得分较低，
然后取两者中的最大值。
-对，所以好吧，所以问题是，
如果我们的图像x像一匹朝左的马
在W-one中，我们有一个模板
一匹向左的马和一匹向右的马，
那会发生什么，对吗？
所以发生的是，所以在h中你可能有
你的左脸马得分很高，
你的右脸马得分有点低，
W-2是一个加权和，所以不是最大值。
这是这些模板的加权和，
但是如果你有一个很高的分数
对于这些模板中的一个，或者说你有，有点
这两个模板的较低和中等分数，
所有这些类型的组合
会给高分，对吗？
所以最终你会得到什么
通常得分很高
当你有一匹马的时候。
假设你有一匹正面朝向的马，
这两个值可能都是中间值
左侧和右侧模板。
嗯，有问题吗？
-[学生]W-2也在称重，
还是h在做加权？
- W-two在做加权，所以问题是，
"是W-2在称重还是h在称重？"
h是这个值，就像这个例子一样，
h是每个模板的得分值
你在W-one有，对吗？
所以h就像分数函数，对吧，
它是W-one中每个模板的数量，
然后W-2会对所有这些进行称重，
对所有这些中间分数进行加权
拿到你的期末成绩。
-[学生]哪个是非线性的东西？
-所以问题是，“哪个是非线性的东西？”
所以非线性通常发生在h之前，
所以h是非线性之后的值。
所以我们在讨论这个，就像，你知道的，
直观地看，这是一个相似的例子，
有人在寻找，你知道，
有和以前一样的模板，
W-2是这些的权重。
实际上不完全是这样的，对吧，
因为就像你说的，有很多
这些非线性等等，
但是它有一种近似的解释。
-[学生]那么h就是W-one-x了？
-是啊，是啊，所以问题是h只是W-one-x？
所以h就是W-1乘以x，上面是max函数。
哦，让我，好吧，
所以我们已经讨论过这个例子
一个两层的神经网络，我们可以堆叠更多层
来得到任意深度的更深的网络，对吗？
所以我们可以再做一次
在另一个非线性和矩阵乘以W-3时，
现在我们有了一个三层神经网络，对吗？
这就是深度神经网络这个术语
基本上是来自于。
你可以堆叠多层这样的想法，
你知道，对于非常深的网络。
所以在家庭作业中，你会得到一个练习
写作，你知道，训练一个
这些神经网络，我想在作业二中，
但基本上这是一个完整的实现
这个向前传球的想法，对吧，
和向后传球，并使用链式法则
来计算我们已经见过的梯度。
两层神经网络的完整实现
实际上非常简单，只用20行就可以完成，
所以你们会在作业二中得到一些练习，
写出所有这些部分。
好了，现在我们已经看到了
神经网络的功能是什么，
就像，你知道，我们听到人们谈论很多关于
神经网络的生物灵感是如何产生的，
所以尽管强调这一点很重要
这些类比真的很松散，
真的只是非常松散的关系，
但是理解起来还是很有趣的
这些联系和灵感从何而来。
所以现在我要简单地谈谈这个问题。
所以如果我们考虑一个神经元，在某种程度上
一个非常简单的方法，这个神经元，
这是一个神经元的示意图。
我们有被带向
每个神经元，对吧，所以我们有很多
神经元连接在一起，每个神经元都有树突，
对，这些是，这些是接收到的
进入神经元的冲动。
然后我们有一个细胞体，对吧，
基本上整合了这些进来的信号
然后有一种，然后它需要这个，
在整合了所有这些信号后，它继续传递，
你知道，冲动带走了
连接到下游的神经元，
对，它通过轴突把这个带走。
所以现在如果我们看看到目前为止我们所做的，对吧，
对于每个计算节点，你可以看到
这实际上有，你可以看到它
有点类似，对吧？
其中节点在
计算图，我们有输入，
或者信号，x，x右，进入一个神经元，
然后所有这些x，对，x-0，x-1，x-2，
这些是结合在一起的，对吧，
例如，使用我们的重量w。
所以我们做了一些计算，对吧，
在我们目前所做的一些计算中，
比如W乘以x加上b，对吧，
将所有这些整合在一起，
然后我们有一个激活函数
我们在上面应用，我们得到这个输出的值，
我们把它传递给连接的神经元。
所以如果你看看这个，这实际上是，
你可以用非常相似的方式来思考这个问题，对吗？
就像，你知道，这些就是进来的信号
是通过突触连接的，对吗？
连接多个神经元的突触，
树突整合了所有这些，
他们将所有这些信息整合在一起
细胞体，然后我们有
这种输出延续了后来的输出。
这是一种类比
你可以在它们之间画画，
如果你观察这些激活函数，对吗？
这基本上就是接收所有的输入
输出一个数字，这个数字稍后会被输出，
我们已经讨论了一些例子
就像乙状结肠激活功能，对吧，
不同种类的非线性，
所以这是一种你可以画出的松散的类比
这些非线性可以代表
有点像解雇，
或者神经元的峰值速率，对吗？
我们的神经元向连接的神经元传递信号
利用这些不连续的尖刺，对吗？
所以我们可以想到，你知道，
如果他们的扣球非常快，那么就有点
一个很强的信号传递给了后来的人，
所以我们可以认为这个值
从某种意义上来说，
我们将要传递的这种发射率。
实际上，我认为神经科学家
他们正在研究这个说法
这是一种非线性
最相似的方式
神经元的实际行为
是一个ReLU函数，它是一个ReLU非线性函数，
这就是我们要去的地方
稍后会看到更多，但这是一个函数
对于所有负值的输入都是零，
然后它是一切的线性函数
这是一种积极的制度。
所以，你知道，我们会谈论更多
这种激活功能后来，
但是实际上，
可能是最类似于
神经元的实际行为。
但是非常小心非常重要
做任何这种大脑类比，
因为实际上生物神经元
要比这复杂得多。
有许多不同种类的生物神经元，
树突可以执行
非常复杂的非线性计算。
我们的突触，对吧，我们之前有的W-0
我们进行类比的地方，并不是单一的重量
就像我们以前一样，它们实际上非常复杂，你知道，
非线性动力系统在实践中，
这种解释我们激活功能的想法
作为一种比率代码或发射率，
在实践中是不够的，你知道。
只是这种射速大概不是
神经元如何
会和下游的神经元交流，对吧，
就像一个非常简单的方法，有一个非常，
神经元会以不同的速率放电，
这种可变性可能应该被考虑在内。
所以有所有这些，你知道，
这是一件更复杂的事情
比我们现在面对的要多。
有参考文献，比如这个树状计算
如果你对这个话题感兴趣，你可以看看，
但是，是的，所以实际上，你知道，
我们可以看到它是如何类似于神经元的
非常高的水平，但是神经元是，
实际上，要比这复杂得多。
好的，我们讨论了有许多不同的种类
可以使用的激活功能，
这是我之前提到的ReLU，
我们会谈到所有这些不同的种类
关于激活功能的更多细节，
这些激活功能的选择
你可能想用的。
所以我们也会谈论不同的种类
神经网络架构。
所以我们举了这些完全连接的例子
神经网络，对，每一层
这个矩阵相乘，所以我们真正想要的方式
我们之前说过两层神经网络，
这与我们有两个
这些线性层，对，我们正在做的
一个矩阵乘法器，两个完全连接层
我们称之为。
我们也可以称之为单隐层神经网络，
所以我们没有数这个数字
我们正在做的矩阵乘法，
计算我们隐藏的层数。
我认为是，你可以用任何一个，
我想也许两层神经网络
是一个更常用的词。
这里也是我们的三层神经网络
那我们有，这也可以叫做
一种双隐层神经网络。
所以我们看到，你知道，当我们做的时候
这种类型的前馈，对，
正向通过神经网络，
这个网络中的每个节点
基本上是在做一种
我之前展示的神经元，对吗？
所以实际上发生的是
基本上是你能想到的每一个隐藏层
作为一个完整的向量，一组神经元，
所以通过这样写出来
用这些矩阵乘法来计算我们的神经元值，
这是一种我们可以有效评估
这一整层神经元，对吗？
所以用一次矩阵乘法我们得到输出值
你知道，一层比如说10层，
或者50或100个神经元。
好的，再看一遍，写下来，
全部以矩阵形式，矩阵向量形式，
你知道，这里有我们的非线性。
我们使用的f，在这种情况下是一个sigmoid函数，对吧，
我们可以把数据x，一些输入向量
或者我们的值，我们可以应用我们的第一个矩阵乘法，
最重要的是，我们的非线性，
然后第二个矩阵相乘得到
第二隐藏层h-2，
然后我们有了最终的输出，对吗？
所以，你知道，这基本上就是你所需要的
为了能够编写一个神经网络，
正如我们之前看到的，向后传球。
你可以用反投影来计算所有这些，
这基本上就是全部了
神经网络的主要概念。
好了，总结一下，我们讨论了
我们如何在这些计算中安排神经元，对吧，
全连接或线性层。
这种抽象层有一个很好的特性
我们可以使用非常有效的矢量化代码
来计算所有这些。
我们也谈到了它的重要性
记住神经网络
确实有一些，你知道，类比和松散的灵感
但它们并不是真正的神经细胞。
我的意思是，这是一个相当松散的类比
我们正在做的，下次我们会谈到
关于卷积神经网络。
好的，谢谢。
